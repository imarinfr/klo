% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/mikl.r
\name{entg}
\alias{entg}
\alias{mig}
\alias{entkl}
\alias{mikl}
\title{Estimates of the differential entropy and mutual information}
\usage{
entg(x)

mig(x, y)

entkl(x, type = "klo", k = 1, p = NULL, w = NULL)

mikl(x, y, type = "klo", k = 1, px = NULL, py = NULL, pxy = NULL, w = NULL)
}
\arguments{
\item{x, y}{n-by-d numeric matrices, in which the n rows correspond to observations
and the d columns to variables (or coordinates) of the multivariate
distributions}

\item{type}{is the type of estimator, \code{"kl"} for the Kozachenko-Leonenko,
\code{"klo"} (default) for its offset version, and \code{"wkl"} and
\code{"wklo"} for the NN weighting versions.}

\item{k}{is the rank of the nearest neighbor for which to search, \code{1},
the nearest neighbor (default), \code{2}, the second nearest,
and so on.}

\item{p, px, py, pxy}{TO BE DEFINED.}

\item{w}{Weights to use for NN weighting.}
}
\description{
Functions that compute the Gaussian and the regular
and offset versions of the Kozachenko-Leonenko estimators of the
differential entropy and mutual information for multivariate
continuous variables
}
\section{Differential entropy estimates}{

\itemize{
  \item\code{entg}  Gaussian estimate of the differential entropy of the
                    multivariate random variables \code{x} and \code{y}.
                    If both \code{x} and \code{y} are multivariate Gaussians,
                    then the estimate is asymptotically unbiased, otherwise
                    it is just an approximation
  \item\code{entkl} The Kozachenko-Leonenko (KL) estimate of the
                    differential entropy of the multivariate random variable
                    \code{x}
}
}

\section{Mutual information entropy estimates}{

\itemize{
  \item\code{mig}   Gaussian estimate of mutual information between the
                    multivariate random variables \code{x} and \code{y}.
                    If both \code{x} and \code{y} are multivariate Gaussians,
                    then the estimate is asymptotically unbiased, otherwise
                    it is just an approximation
  \item\code{mikl}  The Kozachenko-Leonenko (KL) estimate of the
                    mutual information of the multivariate random variable
                    \code{x}
}
}

\examples{
# Generate values from two random Gaussian vectors with different standard deviations
n      <- 10000 # sample size
sdx    <- 3 # standard deviation of the Gaussian
sdy    <- 5 # standard deviation of the Gaussian
x      <- rnorm(n, 0, sdx)
y      <- rnorm(n, 0, sdy)
# theoretical results
exth   <- 1 / 2 * log2((2 * pi * exp(1)) * sdx^2) # theoretical differential entropy
eyth   <- 1 / 2 * log2((2 * pi * exp(1)) * sdy^2) # theoretical differential entropy
mith   <- 0 # the theoretical mutual information is zero as x and y are independently generated
# Gaussian estimates
exg    <- entg(x)   # differential entropy for x
eyg    <- entg(y)   # differential entropy for y
mixyg  <- mig(x, y) # mutual information between x and y
# Kozachenko-Leonenko estimates
exkl   <- entkl(x)   # differential entropy for x
eykl   <- entkl(y)   # differential entropy for y
mixykl <- mikl(x, y) # mutual information between x and y
}
